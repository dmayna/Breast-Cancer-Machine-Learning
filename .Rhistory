split = sample.split(dataset$Class,SplitRatio=0.8)
training_set = subset(dataset, split ==TRUE)
test_set = subset(dataset, split ==FALSE)
# visualizing the training set reslults
#install.packages('ElemStatLearn')
library(ElemStatLearn)
dataset = read.csv('breastCancerWisconsin.csv')
dataset <- subset(dataset, select = -c(1))
names(dataset) <- c("Clump Thickness","Cell Size", "Cell Shape", "Marginal Adhesion", "Epithelial Cell Size", "Bare Nuclei", "Bland Chromatin","Normal Nucleoli","Mitoses","Class")
#check if any empty values
indx <- apply(dataset, 2, function(x) any(is.na(x) | is.infinite(x)))
#(2 for benign, 4 for malignant)
dataset$Class = ifelse(dataset$Class == 2,1,0)
#(1 for benign, 0 for malignant)
# splitting the dataset into training set and test set
#install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Class,SplitRatio=0.8)
training_set = subset(dataset, split ==TRUE)
test_set = subset(dataset, split ==FALSE)
# Fitting data to training set
#install.packages('e1071')
library(e1071)
classifier = svm(formula = Class~.,
data = training_set,
type = 'C-classification',
kernel = 'linear')
# Predicting the Test set results
prob_pred = predict(classifier,type = 'response', test_set[-10])
y_pred = predict(classifier,newdata = test_set[-10])
# making the confusion matrix
cm = table(test_set[,10],y_pred)
####### Decision Tree #####################
#install.packages('rpart')
library(rpart)
treeClassifier = rpart(formula = Class ~.,
data = training_set)
treeY_pred = predict(treeClassifier,newdata = test_set[-10],type='class')
tree_cm = table(test_set[,10],treeY_pred)
plot(treeClassifier)
text(treeClassifier)
#0.655 confidence
############# Random forest ################
#install.packages('randomForest')
library(randomForest)
forestClassifier = randomForest(x = training_set[-10],
y = training_set$Class,
ntree = 10)
y_pred_forest = predict(forestClassifier,newdata = test_set[-10])
cm_forest = table(test_set[,10],y_pred_forest)
# visualizing the training set reslults
#install.packages('ElemStatLearn')
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[,1])-1,max(set[,1])+1, by = 0.01)
X2 = seq(min(set[,10])-1,max(set[,10])+1, by = 0.01)
grid_set = expand.grid(X1,X2)
colnames(grid_set) = c('Clump Thickness','Class')
prob_set = predict(classifier,type = 'response',newdata = grid_set)
y_grid = ifelse(prob_set > 0.5, 1, 0)
plot(set[,10],
main = 'Logistic Regresion (Training Set)',
xlab = "Clump Thickness", ylab = 'Class',
xlim = range(X1), ylim = range(X2))
contour(X1,X2,matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1,'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[,10] == 1,'green4', 'red3'))
set = test_set
X1 = seq(min(set[,1])-1,max(set[,1])+1, by = 0.01)
X2 = seq(min(set[,10])-1,max(set[,10])+1, by = 0.01)
grid_set = expand.grid(X1,X2)
colnames(grid_set) = c('Clump Thickness','Class')
prob_set = predict(classifier,type = 'response',newdata = grid_set)
y_grid = ifelse(prob_set > 0.5, 1, 0)
plot(set[,10],
main = 'Logistic Regresion (Training Set)',
xlab = "Clump Thickness", ylab = 'Class',
xlim = range(X1), ylim = range(X2))
contour(X1,X2,matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1,'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[,10] == 1,'green4', 'red3'))
# other visuals
library(ggplot2)
#p=ggplot(dataset,aes(y =dataset["Clump Thickness"] ,x=seq.int(nrow(dataset)),color=as.factor(Class), shape = Mitoses ,size = dataset["Cell Size"])) +
# geom_point(fill="blue") + scale_shape_identity() + scale_shape_discrete(solid=TRUE, legend=TRUE)+ xlab("Clump Thickness") +
#ylab("ID Number")
#+theme(legend.position="top")
p = ggplot(dataset, aes(x=dataset["Clump Thickness"], y=seq.int(nrow(dataset)))) +
geom_point(aes(shape=Mitoses, color=as.factor(Class), size=dataset["Cell Size"]))+
scale_shape_manual(values=c(3, 16, 17))+
scale_color_manual(values=c('#78c3f0','#d20a0a'))+
theme(legend.position="top") +
scale_shape_identity() +
xlab("Clump Thickness") +
ylab("ID Number")
#+
# scale_shape_manual(name = "Legend",
#                   labels = c(1,2,3,4,5,6,7,8,9,10),
#                  values = c(1,2,3,4,5,6,7,8,9,10))
p
dataset$'Bare Nuclei'
dataset[158]
is.na(dataset$`Bare Nuclei`)
df[df=="?"]
dataset[dataset$`Bare Nuclei` == "?"]
dataset$`Bare Nuclei`[dataset$`Bare Nuclei` == "?"]
dataset$`Bare Nuclei`[dataset$`Clump Thickness` == "?"]
dataset$`Bare Nuclei`[dataset$`Cell Size` == "?"]
length(dataset$`Bare Nuclei`[dataset$`Bare Nuclei` == "?"])
length(dataset$`Bare Nuclei`[dataset$`Clump Thickness` == "?"])
length(dataset$`Bare Nuclei`[dataset$`Cell Size` == "?"])
length(dataset$`Bare Nuclei`[dataset$`Cell Shape` == "?"])
length(dataset$`Bare Nuclei`[dataset$`Marginal Adhesion` == "?"])
length(dataset$`Bare Nuclei`[dataset$`Epithelial Cell Size` == "?"])
length(dataset$`Bare Nuclei`[dataset$`Bland Chromatin` == "?"])
length(dataset$`Bare Nuclei`[dataset$`Normal Nucleoli` == "?"])
length(dataset$`Bare Nuclei`[dataset$Mitoses == "?"])
length(dataset$`Bare Nuclei`[dataset$Class == "?"])
dataset$`Bare Nuclei` = ifelse(dataset$`Bare Nuclei` == "?",ave(dataset$`Bare Nuclei`, FUN = function(x) mean(x,na.rm = TRUE)), dataset$`Bare Nuclei`)
length(dataset$`Bare Nuclei`[dataset$Class == "?"])
dataset = read.csv('breastCancerWisconsin.csv')
dataset <- subset(dataset, select = -c(1))
names(dataset) <- c("Clump Thickness","Cell Size", "Cell Shape", "Marginal Adhesion", "Epithelial Cell Size", "Bare Nuclei", "Bland Chromatin","Normal Nucleoli","Mitoses","Class")
#check if any empty values
indx <- apply(dataset, 2, function(x) any(is.na(x) | is.infinite(x)))
dataset$`Bare Nuclei` = ifelse(dataset$`Bare Nuclei` == "?",
ave(dataset$`Bare Nuclei`, FUN = function(x) mean(x,na.rm = TRUE)),
dataset$`Bare Nuclei`)
#(2 for benign, 4 for malignant)
dataset$Class = ifelse(dataset$Class == 2,1,0)
#(1 for benign, 0 for malignant)
# splitting the dataset into training set and test set
#install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Class,SplitRatio=0.8)
training_set = subset(dataset, split ==TRUE)
test_set = subset(dataset, split ==FALSE)
# Fitting data to training set
#install.packages('e1071')
library(e1071)
classifier = svm(formula = Class~.,
data = training_set,
type = 'C-classification',
kernel = 'linear')
# Predicting the Test set results
prob_pred = predict(classifier,type = 'response', test_set[-10])
y_pred = predict(classifier,newdata = test_set[-10])
# making the confusion matrix
cm = table(test_set[,10],y_pred)
####### Decision Tree #####################
#install.packages('rpart')
library(rpart)
treeClassifier = rpart(formula = Class ~.,
data = training_set)
treeY_pred = predict(treeClassifier,newdata = test_set[-10],type='class')
tree_cm = table(test_set[,10],treeY_pred)
plot(treeClassifier)
text(treeClassifier)
#0.655 confidence
############# Random forest ################
#install.packages('randomForest')
library(randomForest)
forestClassifier = randomForest(x = training_set[-10],
y = training_set$Class,
ntree = 10)
y_pred_forest = predict(forestClassifier,newdata = test_set[-10])
cm_forest = table(test_set[,10],y_pred_forest)
# visualizing the training set reslults
#install.packages('ElemStatLearn')
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[,1])-1,max(set[,1])+1, by = 0.01)
X2 = seq(min(set[,10])-1,max(set[,10])+1, by = 0.01)
grid_set = expand.grid(X1,X2)
colnames(grid_set) = c('Clump Thickness','Class')
prob_set = predict(classifier,type = 'response',newdata = grid_set)
y_grid = ifelse(prob_set > 0.5, 1, 0)
plot(set[,10],
main = 'Logistic Regresion (Training Set)',
xlab = "Clump Thickness", ylab = 'Class',
xlim = range(X1), ylim = range(X2))
contour(X1,X2,matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1,'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[,10] == 1,'green4', 'red3'))
set = test_set
X1 = seq(min(set[,1])-1,max(set[,1])+1, by = 0.01)
X2 = seq(min(set[,10])-1,max(set[,10])+1, by = 0.01)
grid_set = expand.grid(X1,X2)
colnames(grid_set) = c('Clump Thickness','Class')
prob_set = predict(classifier,type = 'response',newdata = grid_set)
y_grid = ifelse(prob_set > 0.5, 1, 0)
plot(set[,10],
main = 'Logistic Regresion (Training Set)',
xlab = "Clump Thickness", ylab = 'Class',
xlim = range(X1), ylim = range(X2))
contour(X1,X2,matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1,'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[,10] == 1,'green4', 'red3'))
# other visuals
library(ggplot2)
#p=ggplot(dataset,aes(y =dataset["Clump Thickness"] ,x=seq.int(nrow(dataset)),color=as.factor(Class), shape = Mitoses ,size = dataset["Cell Size"])) +
# geom_point(fill="blue") + scale_shape_identity() + scale_shape_discrete(solid=TRUE, legend=TRUE)+ xlab("Clump Thickness") +
#ylab("ID Number")
#+theme(legend.position="top")
p = ggplot(dataset, aes(x=dataset["Clump Thickness"], y=seq.int(nrow(dataset)))) +
geom_point(aes(shape=Mitoses, color=as.factor(Class), size=dataset["Cell Size"]))+
scale_shape_manual(values=c(3, 16, 17))+
scale_color_manual(values=c('#78c3f0','#d20a0a'))+
theme(legend.position="top") +
scale_shape_identity() +
xlab("Clump Thickness") +
ylab("ID Number")
#+
# scale_shape_manual(name = "Legend",
#                   labels = c(1,2,3,4,5,6,7,8,9,10),
#                  values = c(1,2,3,4,5,6,7,8,9,10))
p
length(dataset$`Bare Nuclei`[dataset$Class == "?"])
length(dataset$`Bare Nuclei`[dataset$`Bare Nuclei` == "?"])
dataset$`Bare Nuclei`[dataset$`Bare Nuclei` == "?"] <- ave(dataset$`Bare Nuclei`, FUN = function(x) mean(x,na.rm = TRUE))
length(dataset$`Bare Nuclei`[dataset$`Bare Nuclei` == "?"])
tree_cm
106/139
109/139
dataset$`Bare Nuclei`[dataset$`Bare Nuclei` == "?"] <- 0
length(dataset$`Bare Nuclei`[dataset$`Bare Nuclei` == "?"])
cm
tree_cm
cm_forest
library(randomForest)
forestClassifier = randomForest(x = training_set[-10],
y = training_set$Class,
ntree = 10)
y_pred_forest = predict(forestClassifier,newdata = test_set[-10])
cm_forest = table(test_set[,10],y_pred_forest)
dataset$`Bare Nuclei`[dataset$`Bare Nuclei` == "?"] <- "0"
length(dataset$`Bare Nuclei`[dataset$`Bare Nuclei` == "?"])
dataset <- dataset.frame(lapply(dataset, function(x) {
+                  gsub("?", "0", x)
+              }))
dataset <- dataset.frame(lapply(dataset, function(x) {gsub("?", "0", x)}))
dataset <- data.frame(lapply(dataset, function(x) {gsub("?", "0", x)}))
length(dataset$`Bare Nuclei`[dataset$`Bare Nuclei` == "?"])
library(randomForest)
forestClassifier = randomForest(x = training_set[-10],
y = training_set$Class,
ntree = 10)
y_pred_forest = predict(forestClassifier,newdata = test_set[-10])
cm_forest = table(test_set[,10],y_pred_forest)
set.seed(123)
split = sample.split(dataset$Class,SplitRatio=0.8)
training_set = subset(dataset, split ==TRUE)
test_set = subset(dataset, split ==FALSE)
library(randomForest)
forestClassifier = randomForest(x = training_set[-10],
y = training_set$Class,
ntree = 10)
y_pred_forest = predict(forestClassifier,newdata = test_set[-10])
cm_forest = table(test_set[,10],y_pred_forest)
cm_forest
ave(dataset$`Bare Nuclei`, FUN = function(x) mean(x,na.rm = TRUE))
ave(dataset$`Bare Nuclei`, FUN = function(x) mean(x,na.rm = FALSE))
mean(dataset$Bare.Nuclei)
dataset <- data.frame(lapply(dataset, function(x) {gsub("?", 0, x)}))
set.seed(123)
split = sample.split(dataset$Class,SplitRatio=0.8)
training_set = subset(dataset, split ==TRUE)
test_set = subset(dataset, split ==FALSE)
library(randomForest)
forestClassifier = randomForest(x = training_set[-10],
y = training_set$Class,
ntree = 10)
y_pred_forest = predict(forestClassifier,newdata = test_set[-10])
cm_forest = table(test_set[,10],y_pred_forest)
cm_forest
mean(dataset$Bare.Nuclei)
length(dataset$`Bare Nuclei`[dataset$`Bare Nuclei` == "?"])
length(is.na(dataset$Bare.Nuclei))
dataset <- data.frame(lapply(dataset, function(x) {gsub("?", as.factor(0), x)}))
length(dataset$`Bare Nuclei`[dataset$`Bare Nuclei` == "?"])
mean(dataset$Bare.Nuclei)
mean(dataset$Cell.Size)
mean(data.matrix(dataset$Bare.Nuclei))
mean(data.matrix(dataset$Cell.Size))
library(class)
knn_y_pred = knn(train = training_set[,-10],
test = test_set[,-10],
cl = training_set[,10],
k = 2)
dataset = read.csv('breastCancerWisconsin.csv')
dataset <- subset(dataset, select = -c(1))
names(dataset) <- c("Clump Thickness","Cell Size", "Cell Shape", "Marginal Adhesion", "Epithelial Cell Size", "Bare Nuclei", "Bland Chromatin","Normal Nucleoli","Mitoses","Class")
#check if any empty values
indx <- apply(dataset, 2, function(x) any(is.na(x) | is.infinite(x)))
dataset$`Bare Nuclei` = ifelse(dataset$`Bare Nuclei` == "?",
ave(dataset$`Bare Nuclei`, FUN = function(x) mean(x,na.rm = TRUE)),
dataset$`Bare Nuclei`)
dataset <- data.frame(lapply(dataset, function(x) {gsub("?", 0, x)}))
#(2 for benign, 4 for malignant)
dataset$Class = ifelse(dataset$Class == 2,1,0)
#(1 for benign, 0 for malignant)
# splitting the dataset into training set and test set
#install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Class,SplitRatio=0.8)
training_set = subset(dataset, split ==TRUE)
test_set = subset(dataset, split ==FALSE)
library(class)
knn_y_pred = knn(train = training_set[,-10],
test = test_set[,-10],
cl = training_set[,10],
k = 2)
is.na(dataset)
length(is.na(dataset))
length(is.na(dataset) == TRUE)
dataset$Bare.Nuclei[is.na(dataset$Bare.Nuclei)] <- 0
dataset$Bare.Nuclei[dataset$Bare.Nuclei == NA] <- 0
is.na(dataset)
dataset[is.na(dataset)] <- 0
dataset[is.na(dataset)] <- as.factor(0)
dataset<- data.frame("Type" = character(3), "Amount" = numeric(3),stringsAsFactors=FALSE)
dataset[is.na(dataset)] <- 0
is.na(dataset)
dataset = read.csv('breastCancerWisconsin.csv')
dataset <- subset(dataset, select = -c(1))
names(dataset) <- c("Clump Thickness","Cell Size", "Cell Shape", "Marginal Adhesion", "Epithelial Cell Size", "Bare Nuclei", "Bland Chromatin","Normal Nucleoli","Mitoses","Class")
is.na(dataset)
library(class)
knn_y_pred = knn(train = training_set[,-10],
test = test_set[,-10],
cl = training_set[,10],
k = 2)
dataset <- data.frame(lapply(dataset, function(x) {gsub("?", 0, x)}))
is.na(dataset)
library(class)
knn_y_pred = knn(train = training_set[,-10],
test = test_set[,-10],
cl = training_set[,10],
k = 2)
dataset[is.na(dataset)] <- 0
library(class)
knn_y_pred = knn(train = training_set[,-10],
test = test_set[,-10],
cl = training_set[,10],
k = 2)
library(class)
knn_y_pred = knn(train = training_set[,-10],
test = test_set[,-10],
cl = training_set[,10],
k = 2)
cm_knn = table(test_set[,10],knn_y_pred)
cm
48+89
137/139
tree_cm
106+53+1+3
163+11
163/174
cm
48+89
137/139
cm_forest
y_pred_forest
cm_forest
test_set[,10]
76/139
87/139
113/139
123/129
119/139
# importing the dataset
dataset = read.csv('breastCancerWisconsin.csv')
dataset <- subset(dataset, select = -c(1))
names(dataset) <- c("Clump Thickness","Cell Size", "Cell Shape", "Marginal Adhesion", "Epithelial Cell Size", "Bare Nuclei", "Bland Chromatin","Normal Nucleoli","Mitoses","Class")
#check if any empty values
indx <- apply(dataset, 2, function(x) any(is.na(x) | is.infinite(x)))
dataset$`Bare Nuclei` = ifelse(dataset$`Bare Nuclei` == "?",
ave(dataset$`Bare Nuclei`, FUN = function(x) mean(x,na.rm = TRUE)),
dataset$`Bare Nuclei`)
dataset <- data.frame(lapply(dataset, function(x) {gsub("?", 0, x)}))
#(2 for benign, 4 for malignant)
dataset$Class = ifelse(dataset$Class == 2,1,0)
#(1 for benign, 0 for malignant)
# splitting the dataset into training set and test set
#install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Class,SplitRatio=0.8)
training_set = subset(dataset, split ==TRUE)
test_set = subset(dataset, split ==FALSE)
# Fitting data to training set
#install.packages('e1071')
library(e1071)
classifier = svm(formula = Class~.,
data = training_set,
type = 'C-classification',
kernel = 'linear')
# Predicting the Test set results
prob_pred = predict(classifier,type = 'response', test_set[-10])
y_pred = predict(classifier,newdata = test_set[-10])
# making the confusion matrix
cm = table(test_set[,10],y_pred)
####### Decision Tree #####################
#install.packages('rpart')
library(rpart)
treeClassifier = rpart(formula = Class ~.,
data = training_set)
treeY_pred = predict(treeClassifier,newdata = test_set[-10],type='class')
tree_cm = table(test_set[,10],treeY_pred)
plot(treeClassifier)
text(treeClassifier)
#0.655 confidence
############# Random forest ################
#install.packages('randomForest')
library(randomForest)
forestClassifier = randomForest(x = training_set[-10],
y = training_set$Class,
ntree = 10)
y_pred_forest = predict(forestClassifier,newdata = test_set[-10])
cm_forest = table(test_set[,10],y_pred_forest)
############### KNN ##############################
library(class)
knn_y_pred = knn(train = training_set[,-10],
test = test_set[,-10],
cl = training_set[,10],
k = 2)
cm_knn = table(test_set[,10],knn_y_pred)
# visualizing the training set reslults
#install.packages('ElemStatLearn')
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[,1])-1,max(set[,1])+1, by = 0.01)
X2 = seq(min(set[,10])-1,max(set[,10])+1, by = 0.01)
grid_set = expand.grid(X1,X2)
colnames(grid_set) = c('Clump Thickness','Class')
prob_set = predict(classifier,type = 'response',newdata = grid_set)
y_grid = ifelse(prob_set > 0.5, 1, 0)
plot(set[,10],
main = 'Logistic Regresion (Training Set)',
xlab = "Clump Thickness", ylab = 'Class',
xlim = range(X1), ylim = range(X2))
contour(X1,X2,matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1,'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[,10] == 1,'green4', 'red3'))
set = test_set
X1 = seq(min(set[,1])-1,max(set[,1])+1, by = 0.01)
X2 = seq(min(set[,10])-1,max(set[,10])+1, by = 0.01)
grid_set = expand.grid(X1,X2)
colnames(grid_set) = c('Clump Thickness','Class')
prob_set = predict(classifier,type = 'response',newdata = grid_set)
y_grid = ifelse(prob_set > 0.5, 1, 0)
plot(set[,10],
main = 'Logistic Regresion (Training Set)',
xlab = "Clump Thickness", ylab = 'Class',
xlim = range(X1), ylim = range(X2))
contour(X1,X2,matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1,'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[,10] == 1,'green4', 'red3'))
# other visuals
library(ggplot2)
#p=ggplot(dataset,aes(y =dataset["Clump Thickness"] ,x=seq.int(nrow(dataset)),color=as.factor(Class), shape = Mitoses ,size = dataset["Cell Size"])) +
# geom_point(fill="blue") + scale_shape_identity() + scale_shape_discrete(solid=TRUE, legend=TRUE)+ xlab("Clump Thickness") +
#ylab("ID Number")
#+theme(legend.position="top")
p = ggplot(dataset, aes(x=dataset["Clump Thickness"], y=seq.int(nrow(dataset)))) +
geom_point(aes(shape=Mitoses, color=as.factor(Class), size=dataset["Cell Size"]))+
scale_shape_manual(values=c(3, 16, 17))+
scale_color_manual(values=c('#78c3f0','#d20a0a'))+
theme(legend.position="top") +
scale_shape_identity() +
xlab("Clump Thickness") +
ylab("ID Number")
#+
# scale_shape_manual(name = "Legend",
#                   labels = c(1,2,3,4,5,6,7,8,9,10),
#                  values = c(1,2,3,4,5,6,7,8,9,10))
p
dataset = read.csv('breastCancerWisconsin.csv')
dataset <- subset(dataset, select = -c(1))
names(dataset) <- c("Clump Thickness","Cell Size", "Cell Shape", "Marginal Adhesion", "Epithelial Cell Size", "Bare Nuclei", "Bland Chromatin","Normal Nucleoli","Mitoses","Class")
indx <- apply(dataset, 2, function(x) any(is.na(x) | is.infinite(x)))
dataset$`Bare Nuclei` = ifelse(dataset$`Bare Nuclei` == "?",
ave(dataset$`Bare Nuclei`, FUN = function(x) mean(x,na.rm = TRUE)),
dataset$`Bare Nuclei`)
dataset <- data.frame(lapply(dataset, function(x) {gsub("?", 0, x)}))
#(2 for benign, 4 for malignant)
dataset$Class = ifelse(dataset$Class == 2,1,0)
p = ggplot(dataset, aes(x=dataset["Clump Thickness"], y=seq.int(nrow(dataset)))) +
geom_point(aes(shape=Mitoses, color=as.factor(Class), size=dataset["Cell Size"]))+
scale_shape_manual(values=c(3, 16, 17))+
scale_color_manual(values=c('#78c3f0','#d20a0a'))+
theme(legend.position="top") +
scale_shape_identity() +
xlab("Clump Thickness") +
ylab("ID Number")
p
p = ggplot(dataset, aes(x=dataset["Clump Thickness"], y=seq.int(nrow(dataset)))) +
geom_point(aes(shape=Mitoses, color=as.factor(Class), size=as.factor(dataset["Cell Size"])))+
scale_shape_manual(values=c(3, 16, 17))+
scale_color_manual(values=c('#78c3f0','#d20a0a'))+
theme(legend.position="top") +
scale_shape_identity() +
xlab("Clump Thickness") +
ylab("ID Number")
p
dataset = read.csv('breastCancerWisconsin.csv')
dataset
dataset <- subset(dataset, select = -c(1))
names(dataset) <- c("Clump Thickness","Cell Size", "Cell Shape", "Marginal Adhesion", "Epithelial Cell Size", "Bare Nuclei", "Bland Chromatin","Normal Nucleoli","Mitoses","Class")
